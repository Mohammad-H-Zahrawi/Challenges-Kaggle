{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammad-H-Zahrawi/Challenges-Kaggle/blob/main/Twitter%20Sentiment%20Analysis-Batch/clusterFinal2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZC-rsltjKpe",
        "outputId": "cd91c49d-8ff5-486b-e296-267b98beba6c"
      },
      "id": "7ZC-rsltjKpe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "3ZznT49w6NfR"
      },
      "id": "3ZznT49w6NfR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newStopWords = {'a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'after',\n",
        " 'again',\n",
        " 'against',\n",
        " 'all',\n",
        " 'am',\n",
        " 'an',\n",
        " 'and',\n",
        " 'any',\n",
        " 'are',\n",
        " 'as',\n",
        " 'at',\n",
        " 'be',\n",
        " 'because',\n",
        " 'been',\n",
        " 'before',\n",
        " 'being',\n",
        " 'below',\n",
        " 'between',\n",
        " 'both',\n",
        " 'but',\n",
        " 'by',\n",
        " 'can',\n",
        " 'd',\n",
        " 'did',\n",
        " 'do',\n",
        " 'does',\n",
        " 'doing',\n",
        " 'down',\n",
        " 'during',\n",
        " 'each',\n",
        " 'few',\n",
        " 'for',\n",
        " 'from',\n",
        " 'further',\n",
        " 'had',\n",
        " 'hadn',\n",
        " 'has',\n",
        " 'have',\n",
        " 'having',\n",
        " 'he',\n",
        " 'her',\n",
        " 'here',\n",
        " 'hers',\n",
        " 'herself',\n",
        " 'him',\n",
        " 'himself',\n",
        " 'his',\n",
        " 'how',\n",
        " 'i',\n",
        " 'if',\n",
        " 'in',\n",
        " 'into',\n",
        " 'is',\n",
        " 'isn',\n",
        " \"isn't\",\n",
        " 'it',\n",
        " \"it's\",\n",
        " 'its',\n",
        " 'itself',\n",
        " 'just',\n",
        " 'll',\n",
        " 'm',\n",
        " 'ma',\n",
        " 'me',\n",
        " 'mightn',\n",
        " 'more',\n",
        " 'most',\n",
        " 'my',\n",
        " 'myself',\n",
        " 'now',\n",
        " 'o',\n",
        " 'of',\n",
        " 'off',\n",
        " 'on',\n",
        " 'once',\n",
        " 'only',\n",
        " 'or',\n",
        " 'other',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'out',\n",
        " 'over',\n",
        " 'own',\n",
        " 're',\n",
        " 's',\n",
        " 'same',\n",
        " 'she',\n",
        " \"she's\",\n",
        " 'should',\n",
        " \"should've\",\n",
        " 'shouldn',\n",
        " 'so',\n",
        " 'some',\n",
        " 'such',\n",
        " 't',\n",
        " 'than',\n",
        " 'that',\n",
        " \"that'll\",\n",
        " 'the',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'them',\n",
        " 'themselves',\n",
        " 'then',\n",
        " 'there',\n",
        " 'these',\n",
        " 'they',\n",
        " 'this',\n",
        " 'those',\n",
        " 'through',\n",
        " 'to',\n",
        " 'too',\n",
        " 'under',\n",
        " 'until',\n",
        " 'up',\n",
        " 've',\n",
        " 'very',\n",
        " 'was',\n",
        " 'wasn',\n",
        " 'we',\n",
        " 'were',\n",
        " 'what',\n",
        " 'when',\n",
        " 'where',\n",
        " 'which',\n",
        " 'while',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'why',\n",
        " 'will',\n",
        " 'with',\n",
        " 'won',\n",
        " 'y',\n",
        " 'you',\n",
        " \"you'd\",\n",
        " \"you'll\",\n",
        " \"you're\",\n",
        " \"you've\",\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves'}"
      ],
      "metadata": {
        "id": "TvmbeJob6PEH"
      },
      "id": "TvmbeJob6PEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78ea5d2-dad1-43bd-8bdb-f185731d7a57",
      "metadata": {
        "id": "b78ea5d2-dad1-43bd-8bdb-f185731d7a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6541a4d-8444-4756-9f34-28ed433ea658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5bef1e73d4e7>:44: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-7-5bef1e73d4e7>:44: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Load data\n",
        "df=pd.read_csv('/content/drive/MyDrive/twitter/train1.csv', encoding='ISO-8859-1')\n",
        "df2=pd.read_csv('/content/drive/MyDrive/twitter/test1.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# df = pd.read_csv(\"train1.csv\", encoding='ISO-8859-1')\n",
        "# df2 = pd.read_csv(\"test1.csv\", encoding='ISO-8859-1')\n",
        "test_ids = df2['id']\n",
        "X_test = df2.copy()\n",
        "df = df.drop(columns=['id', 'user', 'date', 'flag'])\n",
        "df2 = df2.drop(columns=['id', 'user', 'date', 'flag'])\n",
        "\n",
        "# Text preprocessing functions\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "    clean_text = re.sub(r'http\\S+', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def replace_chat_words(text):\n",
        "    chat_words = {\n",
        "        \"BRB\": \"Be right back\",\n",
        "        \"BTW\": \"By the way\",\n",
        "        \"OMG\": \"Oh my God/goodness\",\n",
        "        \"TTYL\": \"Talk to you later\",\n",
        "        \"OMW\": \"On my way\",\n",
        "        \"SMH/SMDH\": \"Shaking my head/shaking my darn head\",\n",
        "        \"LOL\": \"Laugh out loud\",\n",
        "        \"TBD\": \"To be determined\",\n",
        "        \"IMHO/IMO\": \"In my humble opinion\",\n",
        "        \"HMU\": \"Hit me up\",\n",
        "        \"IIRC\": \"If I remember correctly\",\n",
        "        \"LMK\": \"Let me know\",\n",
        "        \"OG\": \"Original gangsters (used for old friends)\",\n",
        "        \"FTW\": \"For the win\",\n",
        "        \"NVM\": \"Nevermind\",\n",
        "        \"OOTD\": \"Outfit of the day\",\n",
        "        \"Ngl\": \"Not gonna lie\",\n",
        "        \"Rq\": \"real quick\",\n",
        "        \"Iykyk\": \"If you know, you know\",\n",
        "        \"Ong\": \"On god (I swear)\",\n",
        "        \"YAAAS\": \"Yes!\",\n",
        "        \"Brt\": \"Be right there\",\n",
        "        \"Sm\": \"So much\",\n",
        "        \"Ig\": \"I guess\",\n",
        "        \"Wya\": \"Where you at\",\n",
        "        \"Istg\": \"I swear to god\",\n",
        "        \"Hbu\": \"How about you\",\n",
        "        \"Atm\": \"At the moment\",\n",
        "        \"Asap\": \"As soon as possible\",\n",
        "        \"Fyi\": \"For your information\"\n",
        "    }\n",
        "    for word, expanded_form in chat_words.items():\n",
        "        text = text.replace(word, expanded_form)\n",
        "    return text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    clean_text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    return clean_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in newStopWords]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_whitespace(text):\n",
        "    return text.strip()\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_urls(text)\n",
        "    text = convert_to_lowercase(text)\n",
        "    text = replace_chat_words(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_special_characters(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "df2['text'] = df2['text'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
        "# Vectorize text using TF-IDF\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=2 )\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=30)\n",
        "kmeans.fit(X_train)\n",
        "# Predict cluster labels\n",
        "y_train_pred = kmeans.predict(X_train)\n",
        "y_test_pred = kmeans.predict(X_test)\n",
        "# Evaluate clustering performance\n",
        "accuracy_train = f1_score(y_train, y_train_pred)\n",
        "accuracy_test = f1_score(y_test, y_test_pred)\n",
        "print(\"Training f1_score:\", accuracy_train)\n",
        "print(\"Test f1_score:\", accuracy_test)"
      ],
      "metadata": {
        "id": "GX7m5BFujqYV"
      },
      "id": "GX7m5BFujqYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_ytrain_labels  = [1 if x==0 else 0 for x in y_train_pred]\n",
        "inverse_ytest_labels  = [1 if x==0 else 0 for x in y_test_pred]\n",
        "accuracy_train = f1_score(y_train, inverse_ytrain_labels)\n",
        "accuracy_test = f1_score(y_test, inverse_ytest_labels)\n",
        "\n",
        "print(\"Training f1_score inverse:\", accuracy_train)\n",
        "print(\"Test f1_score inverse\", accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNs_IXW-kVSO",
        "outputId": "51af64cb-0193-41aa-e70d-2036cf74ebce"
      },
      "id": "aNs_IXW-kVSO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training f1_score inverse: 0.652778469624157\n",
            "Test f1_score inverse 0.6559970989933647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "X_scled_test = scaler.fit_transform(X_test)\n",
        "\n",
        "brc = Birch(branching_factor=50, n_clusters=2, threshold=0.5, compute_labels=True)\n"
      ],
      "metadata": {
        "id": "ile4woDFlU9c"
      },
      "id": "ile4woDFlU9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brc.fit(X_scaled)"
      ],
      "metadata": {
        "id": "-VAHRA6JoYHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319c89aa-235d-43d4-b145-323983e42c0d"
      },
      "id": "-VAHRA6JoYHQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7c7b43fc5750>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nltk/data.py\", line 1160, in __del__\n",
            "    if not self.closed:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nltk/data.py\", line 1180, in closed\n",
            "    return self.stream.closed\n",
            "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_labels = brc.predict(X_scaled)"
      ],
      "metadata": {
        "id": "pwNtxaJ-lU68"
      },
      "id": "pwNtxaJ-lU68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# osama"
      ],
      "metadata": {
        "id": "0MfaXChYlUpl"
      },
      "id": "0MfaXChYlUpl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c4d7d9-14f4-448e-adcf-91a61fa6dc4f",
      "metadata": {
        "id": "e3c4d7d9-14f4-448e-adcf-91a61fa6dc4f"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=30)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Predict cluster labels\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "# Transform the new dataset using the same vectorizer\n",
        "X_test_new = vectorizer.transform(df2['text'])\n",
        "\n",
        "# Predict clusters for the new dataset\n",
        "predictions = kmeans.predict(X_test_new)\n",
        "# Create a DataFrame for the predictions and save to CSV\n",
        "results = pd.DataFrame({'id': test_ids, 'predicted_cluster': predictions})\n",
        "results.to_csv('clust3.csv', index=False)\n",
        "\n",
        "print(results.head())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}