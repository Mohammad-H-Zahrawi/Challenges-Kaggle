{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/eBcNTPrk+gxuM0IvJk07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammad-H-Zahrawi/Challenges-Kaggle/blob/main/Twitter%20Sentiment%20Analysis-Batch/twitter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob3xc5Imbprh",
        "outputId": "a4ffbf30-953c-46c3-ae54-497f26bf538d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, fbeta_score, make_scorer, recall_score\n",
        "rs = make_scorer(recall_score)\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "train=pd.read_csv('/content/drive/MyDrive/twitter/train1.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/twitter/test1.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove user name @mohammmad\n",
        "pattern = r'@\\w+'\n",
        "train['text'] = train['text'].apply(lambda x: re.sub(pattern, '', x))\n",
        "test['text'] = test['text'].apply(lambda x: re.sub(pattern, '', x))"
      ],
      "metadata": {
        "id": "Nwjp0pL1dEud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword = set(stopwords.words('english'))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "userPattern = '@[^\\s]+'\n",
        "some = 'amp,today,tomorrow,going,girl'\n",
        "def process_tweets(tweet):\n",
        "  # Lower Casing\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n",
        "\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
        "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
        "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
        "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
        "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
        "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
        "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
        "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
        "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
        "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
        "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
        "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
        "    tweet = re.sub(r\"goood\", \"good\", tweet)\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    return tweet\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x: process_tweets(x))\n",
        "test['text'] = test['text'].apply(lambda x: process_tweets(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5NkJWmEdcHw",
        "outputId": "259ef187-a3ca-43f9-c8cd-7f1012422536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "     \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "def convert_abbrev_in_text(tweet):\n",
        "    t=[]\n",
        "    words=tweet.split()\n",
        "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
        "    return ' '.join(t)\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x: convert_abbrev_in_text(x))\n",
        "test['text'] = test['text'].apply(lambda x: convert_abbrev_in_text(x))"
      ],
      "metadata": {
        "id": "omxsF1R2eCtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(x):\n",
        "    return x.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x:remove_punc(x))\n",
        "test['text'] = test['text'].apply(lambda x:remove_punc(x))\n"
      ],
      "metadata": {
        "id": "-WapmqImfXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = {'a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'after',\n",
        " 'again',\n",
        " 'against',\n",
        " 'all',\n",
        " 'am',\n",
        " 'an',\n",
        " 'and',\n",
        " 'any',\n",
        " 'are',\n",
        " 'as',\n",
        " 'at',\n",
        " 'be',\n",
        " 'because',\n",
        " 'been',\n",
        " 'before',\n",
        " 'being',\n",
        " 'below',\n",
        " 'between',\n",
        " 'both',\n",
        " 'but',\n",
        " 'by',\n",
        " 'can',\n",
        " 'couldn',\n",
        " \"couldn't\",\n",
        " 'd',\n",
        " 'did',\n",
        " 'do',\n",
        " 'does',\n",
        " 'doing',\n",
        " 'don',\n",
        " 'down',\n",
        " 'during',\n",
        " 'each',\n",
        " 'few',\n",
        " 'for',\n",
        " 'from',\n",
        " 'further',\n",
        " 'had',\n",
        " 'has',\n",
        " 'hasn',\n",
        " 'have',\n",
        " 'haven',\n",
        " \"haven't\",\n",
        " 'having',\n",
        " 'he',\n",
        " 'her',\n",
        " 'here',\n",
        " 'hers',\n",
        " 'herself',\n",
        " 'him',\n",
        " 'himself',\n",
        " 'his',\n",
        " 'how',\n",
        " 'i',\n",
        " 'if',\n",
        " 'in',\n",
        " 'into',\n",
        " 'is',\n",
        " 'it',\n",
        " \"it's\",\n",
        " 'its',\n",
        " 'itself',\n",
        " 'just',\n",
        " 'll',\n",
        " 'm',\n",
        " 'ma',\n",
        " 'me',\n",
        " 'mightn',\n",
        " 'more',\n",
        " 'most',\n",
        " 'mustn',\n",
        " \"mustn't\",\n",
        " 'my',\n",
        " 'myself',\n",
        " 'needn',\n",
        " 'now',\n",
        " 'o',\n",
        " 'of',\n",
        " 'off',\n",
        " 'on',\n",
        " 'once',\n",
        " 'only',\n",
        " 'or',\n",
        " 'other',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'out',\n",
        " 'over',\n",
        " 'own',\n",
        " 're',\n",
        " 's',\n",
        " 'same',\n",
        " 'shan',\n",
        " \"shan't\",\n",
        " 'she',\n",
        " \"she's\",\n",
        " 'should',\n",
        " \"should've\",\n",
        " 'so',\n",
        " 'some',\n",
        " 'such',\n",
        " 't',\n",
        " 'than',\n",
        " 'that',\n",
        " \"that'll\",\n",
        " 'the',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'them',\n",
        " 'themselves',\n",
        " 'then',\n",
        " 'there',\n",
        " 'these',\n",
        " 'they',\n",
        " 'this',\n",
        " 'those',\n",
        " 'through',\n",
        " 'to',\n",
        " 'too',\n",
        " 'under',\n",
        " 'until',\n",
        " 'up',\n",
        " 've',\n",
        " 'very',\n",
        " 'was',\n",
        " 'we',\n",
        " 'were',\n",
        " 'weren',\n",
        " 'what',\n",
        " 'when',\n",
        " 'where',\n",
        " 'which',\n",
        " 'while',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'why',\n",
        " 'will',\n",
        " 'with',\n",
        " 'won',\n",
        " 'y',\n",
        " 'you',\n",
        " \"you'd\",\n",
        " \"you'll\",\n",
        " \"you're\",\n",
        " \"you've\",\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves'}\n"
      ],
      "metadata": {
        "id": "3A6TVnnvfYF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeStopWords(x):\n",
        "    tokens = word_tokenize(x)\n",
        "    final_tokens = [w for w in tokens if w not in stopwords]\n",
        "    return ' '.join(final_tokens)\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x:removeStopWords(x))\n",
        "test['text'] = test['text'].apply(lambda x:removeStopWords(x))\n"
      ],
      "metadata": {
        "id": "vi-i2I22hb6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove whitespace\n",
        "def remove_whitespace(text):\n",
        "    return text.strip()\n",
        "\n",
        "# Function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def convert_to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    clean_text = re.sub(r'http\\S+', '', text)\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_urls(text)\n",
        "    text = convert_to_lowercase(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_special_characters(text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing function to DataFrame\n",
        "train['text'] = train['text'].apply(preprocess_text)\n",
        "test['text'] = test['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "6Glu5D30jLiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_patterns = [\n",
        "        (b'US', b'United States'),\n",
        "        (b'IT', b'Information Technology'),\n",
        "        (b'(W|w)on\\'t', b'will not'),\n",
        "        (b'(C|c)an\\'t', b'can not'),\n",
        "        (b'(I|i)\\'m', b'i am'),\n",
        "        (b'(A|a)in\\'t', b'is not'),\n",
        "        (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
        "        (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
        "        (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
        "        (b'(\\w+)\\'s', b'\\g<1> is'),\n",
        "        (b'(\\w+)\\'re', b'\\g<1> are'),\n",
        "        (b'(\\w+)\\'d', b'\\g<1> would'),\n",
        "    ]\n",
        "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
        "\n",
        "\n",
        "def prepare_for_char_n_gram(text):\n",
        "    \"\"\" Simple text clean up process\"\"\"\n",
        "    # 1. Go to lower case (only good for english)\n",
        "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
        "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
        "    # 2. Drop \\n and  \\t\n",
        "    clean = clean.replace(b\"\\n\", b\" \")\n",
        "    clean = clean.replace(b\"\\t\", b\" \")\n",
        "    clean = clean.replace(b\"\\b\", b\" \")\n",
        "    clean = clean.replace(b\"\\r\", b\" \")\n",
        "    # 3. Replace english contractions\n",
        "    for (pattern, repl) in patterns:\n",
        "        clean = re.sub(pattern, repl, clean)\n",
        "    # 4. Drop puntuation\n",
        "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
        "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
        "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
        "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
        "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
        "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
        "    clean = re.sub(b'\\s+', b' ', clean)\n",
        "    # Remove ending space if any\n",
        "    clean = re.sub(b'\\s+$', b'', clean)\n",
        "    # 7. Now replace words by words surrounded by # signs\n",
        "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
        "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
        "    # clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
        "    # clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
        "\n",
        "    return str(clean, 'utf-8')\n",
        "\n",
        "def count_regexp_occ(regexp=\"\", text=None):\n",
        "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
        "    return len(re.findall(regexp, text))\n",
        "\n",
        "train['text'] = train['text'].apply(lambda x:prepare_for_char_n_gram(x))\n",
        "test['text'] = test['text'].apply(lambda x:prepare_for_char_n_gram(x))\n",
        "jigsaw['text'] = jigsaw['text'].apply(lambda x:prepare_for_char_n_gram(x))\n",
        "balanced['text'] = balanced['text'].apply(lambda x:prepare_for_char_n_gram(x))"
      ],
      "metadata": {
        "id": "Czs4Sy7GknTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_evaluation_scores(y_val, predicted):\n",
        "    print('ROC AUC:',roc_auc_score(y_val, predicted))\n",
        "    print('Accuracy:',accuracy_score(y_val, predicted))\n",
        "    print('Precision:',average_precision_score(y_val, predicted,average='weighted'))\n",
        "    print('Recall:',recall_score(y_val, predicted,average='weighted'))\n",
        "    print('F1 Score:',f1_score(y_val, predicted, average='weighted'))\n"
      ],
      "metadata": {
        "id": "f_MeWNwnhc3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF1\n",
        "#split data\n",
        "X = df['text']\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "test = vectorizer.transform(df2['text'])"
      ],
      "metadata": {
        "id": "h_MQ3Fui3bh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_train = svd.fit_transform(X_train)\n",
        "X_test = svd.fit_transform(X_test)\n",
        "\n",
        "#Kmeans with scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler(with_mean=False, with_std =False)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler(with_centering=False)\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans1 = KMeans(n_clusters=2)\n",
        "kmeans1.fit(X_train)\n",
        "# Predict cluster labels\n",
        "y_train_pred = kmeans1.predict(X_train)\n",
        "y_test_pred = kmeans1.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate clustering performance\n",
        "accuracy_train = f1_score(y_train, y_train_pred)\n",
        "accuracy_test = f1_score(y_test, y_test_pred)\n",
        "print(\"Training f1_score:\", accuracy_train)\n",
        "print(\"Test f1_score:\", accuracy_test)\n",
        "\n",
        "inverse_ytrain_labels  = [1 if x==0 else 0 for x in y_train_pred]\n",
        "inverse_ytest_labels  = [1 if x==0 else 0 for x in y_test_pred]\n",
        "accuracy_train = f1_score(y_train, inverse_ytrain_labels)\n",
        "accuracy_test = f1_score(y_test, inverse_ytest_labels)\n",
        "print(\"Training f1_score inverse labels:\", accuracy_train)\n",
        "print(\"Test f1_score inverse labels\", accuracy_test)"
      ],
      "metadata": {
        "id": "xH554qR6eTek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}